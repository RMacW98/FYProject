{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import shrimpy\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseClient:\n",
    "    \"\"\"\n",
    "    Functionality for inserting and reading from the database\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Connect to db\n",
    "        self.con = psycopg2.connect(\n",
    "            host='localhost',\n",
    "            database='postgres',\n",
    "            user='postgres',\n",
    "            password='postgres'\n",
    "        )\n",
    "        self.article_table = pd.DataFrame()\n",
    "\n",
    "    def read_db(self):\n",
    "        cur = self.con.cursor()\n",
    "\n",
    "        cur.execute(\"SELECT timestamp, comp_sentiment, sma, ema FROM sent_values WHERE timestamp > '2021-01-26 15:00:00'::timestamp;\")\n",
    "\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        self.article_table = pd.DataFrame(data=rows, columns=['timestamp','compound','sma','ema'])\n",
    "\n",
    "\n",
    "        return self.article_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_prices(trading_symbol):\n",
    "    public_key = '12326758a39a720e15d064cab3c1f0a9332d107de453bd41926bb3acd565059e'\n",
    "    secret_key = '6991cf4c9b518293429db0df6085d1731074bed8abccd7f0279a52fac5b0c1a8a2f6d28e11a50fbb1c6575d1407e637f9ad7c73fbddfa87c5d418fd58971f829'\n",
    "    \n",
    "    client = shrimpy.ShrimpyApiClient(public_key, secret_key)\n",
    "        \n",
    "    # get the candles for historical values\n",
    "    candles = client.get_candles(\n",
    "        'binance',  # exchange\n",
    "        trading_symbol,      # base_trading_symbol\n",
    "        'USDT',      # quote_trading_symbol\n",
    "        '1h'       # interval\n",
    "    )\n",
    "        \n",
    "    # Set the dataframe between these two dates\n",
    "    tomorrows_date = (datetime.today() - timedelta(hours=1)).strftime(\"%Y-%m-%d %H:00\")\n",
    "    tomorrows_date = (datetime.today()).strftime(\"%Y-%m-%d %H:00\")\n",
    "    starting_date = '2021-01-25 02:00:00'\n",
    "\n",
    "    # Put pulled cryptocurrency values into a dataframe and set dates\n",
    "    prices_df = pd.DataFrame(candles)\n",
    "    prices_df['time'] = pd.to_datetime(prices_df['time'], infer_datetime_format=True).dt.tz_localize(None)\n",
    "\n",
    "    latest_prices = prices_df[(prices_df['time'] > starting_date) & (prices_df['time'] <tomorrows_date)]\n",
    "    latest_prices['close'] = latest_prices['close'].astype('float64')\n",
    "\n",
    "    return latest_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b1d27c6bb073>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  latest_prices['close'] = latest_prices['close'].astype('float64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "close    897\n",
       "sma      897\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_client = DatabaseClient()\n",
    "df = database_client.read_db()\n",
    "df.index = df.timestamp\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "data = get_latest_prices('BTC')\n",
    "data.index = data.time\n",
    "data = data.iloc[:-4]\n",
    "\n",
    "dataset_train = data.join(df, lsuffix=data.index, rsuffix=df.index)\n",
    "\n",
    "dataset_train = dataset_train[['close', 'sma']]\n",
    "dataset_train = dataset_train.dropna()\n",
    "stk_data = dataset_train.astype('float')\n",
    "\n",
    "stk_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "stk_data['Date'] = stk_data.index\n",
    "data2 = pd.DataFrame(columns = ['Date', 'Close'])\n",
    "data2['Date'] = stk_data['Date']\n",
    "data2['Close'] = stk_data['close']\n",
    "data2['SMA'] = stk_data['sma']\n",
    "\n",
    "train_set = data2.iloc[:750, 1:3].values\n",
    "valid_set = data2.iloc[750:, 1:3].values\n",
    "\n",
    "sc_train = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc_train.fit_transform(train_set)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, len(train_set)):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0]) \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "sc_valid = MinMaxScaler(feature_range = (0, 1))\n",
    "valid_set_scaled = sc_valid.fit_transform(valid_set)\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for i in range(60, len(valid_set)):\n",
    "    X_valid.append(valid_set_scaled[i-60:i, 0])\n",
    "    y_valid.append(valid_set_scaled[i, 0]) \n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)\n",
    "X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.468276e+04, 2.252000e-01],\n",
       "       [4.637487e+04, 1.957000e-01],\n",
       "       [4.676822e+04, 1.882000e-01],\n",
       "       ...,\n",
       "       [5.878149e+04, 6.545000e-01],\n",
       "       [5.778976e+04, 6.371000e-01],\n",
       "       [5.775770e+04, 6.371000e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(Dense(units = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 15s 418ms/step - loss: 0.1205 - val_loss: 0.0487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04868, saving model to weights.h5\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0283 - val_loss: 0.0627\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04868\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0204 - val_loss: 0.0569\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04868\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0155 - val_loss: 0.0551\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04868\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0173 - val_loss: 0.0542\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04868\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0133 - val_loss: 0.0537\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04868\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0131 - val_loss: 0.0530\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04868\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0116 - val_loss: 0.0493\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04868\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0124 - val_loss: 0.0463\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04868 to 0.04626, saving model to weights.h5\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0114 - val_loss: 0.0474\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04626\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0088 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04626 to 0.04484, saving model to weights.h5\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0108 - val_loss: 0.0402\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04484 to 0.04022, saving model to weights.h5\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0109 - val_loss: 0.0405\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04022\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0082 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04022 to 0.03668, saving model to weights.h5\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0090 - val_loss: 0.0346\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03668 to 0.03460, saving model to weights.h5\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0086 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03460\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0085 - val_loss: 0.0326\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03460 to 0.03262, saving model to weights.h5\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0068 - val_loss: 0.0276\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03262 to 0.02756, saving model to weights.h5\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0069 - val_loss: 0.0279\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02756\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0078 - val_loss: 0.0258\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02756 to 0.02583, saving model to weights.h5\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0066 - val_loss: 0.0271\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02583\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0067 - val_loss: 0.0241\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02583 to 0.02405, saving model to weights.h5\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0073 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02405 to 0.02177, saving model to weights.h5\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0055 - val_loss: 0.0217\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02177 to 0.02167, saving model to weights.h5\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0056 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02167 to 0.02074, saving model to weights.h5\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0069 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02074 to 0.01909, saving model to weights.h5\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0057 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01909\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0050 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01909 to 0.01758, saving model to weights.h5\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0051 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01758\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0048 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01758\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0050 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01758\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0058 - val_loss: 0.0164\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01758 to 0.01639, saving model to weights.h5\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0051 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01639 to 0.01560, saving model to weights.h5\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0044 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01560 to 0.01466, saving model to weights.h5\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0048 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01466\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0056 - val_loss: 0.0154\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01466\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0052 - val_loss: 0.0137\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.01466 to 0.01372, saving model to weights.h5\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0047 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01372 to 0.01236, saving model to weights.h5\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 1s 61ms/step - loss: 0.0049 - val_loss: 0.0133\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01236\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.0048 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01236 to 0.01236, saving model to weights.h5\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0048 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01236\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 1s 60ms/step - loss: 0.0045 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01236 to 0.01234, saving model to weights.h5\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 1s 61ms/step - loss: 0.0042 - val_loss: 0.0119\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01234 to 0.01192, saving model to weights.h5\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0042 - val_loss: 0.0129\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01192\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0046 - val_loss: 0.0118\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.01192 to 0.01178, saving model to weights.h5\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0039 - val_loss: 0.0106\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01178 to 0.01061, saving model to weights.h5\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0039 - val_loss: 0.0109\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01061\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0038 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01061\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0040 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01061\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0035 - val_loss: 0.0102\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.01061 to 0.01019, saving model to weights.h5\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0038 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01019\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0048 - val_loss: 0.0114\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01019\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0036 - val_loss: 0.0102\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.01019 to 0.01018, saving model to weights.h5\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0034 - val_loss: 0.0099\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.01018 to 0.00988, saving model to weights.h5\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0035 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00988 to 0.00954, saving model to weights.h5\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0036 - val_loss: 0.0110\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00954\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0034 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00954 to 0.00939, saving model to weights.h5\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0040 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00939\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0030 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00939 to 0.00911, saving model to weights.h5\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0033 - val_loss: 0.0092\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00911\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0036 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00911\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0035 - val_loss: 0.0084\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00911 to 0.00843, saving model to weights.h5\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0032 - val_loss: 0.0090\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00843\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0037 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00843\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.0030 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00843\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0029 - val_loss: 0.0080\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00843 to 0.00798, saving model to weights.h5\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0031 - val_loss: 0.0084\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00798\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0032 - val_loss: 0.0082\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00798\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0030 - val_loss: 0.0081\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00798\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00798\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0032 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00798\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 0.0029 - val_loss: 0.0078\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00798 to 0.00783, saving model to weights.h5\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0032 - val_loss: 0.0079\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00783\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0027 - val_loss: 0.0081\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00783\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.0030 - val_loss: 0.0080\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00783\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0028 - val_loss: 0.0079\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00783\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0030 - val_loss: 0.0072\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00783 to 0.00720, saving model to weights.h5\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0028 - val_loss: 0.0073\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00720\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0032 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00720\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00720\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0029 - val_loss: 0.0075\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00720\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00720\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0031 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00720\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00720\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0027 - val_loss: 0.0071\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00720 to 0.00709, saving model to weights.h5\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0028 - val_loss: 0.0092\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00709\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0031 - val_loss: 0.0072\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00709\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0026 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00709 to 0.00683, saving model to weights.h5\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0022 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00683 to 0.00676, saving model to weights.h5\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00676\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0022 - val_loss: 0.0066\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00676 to 0.00665, saving model to weights.h5\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0066\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00665 to 0.00661, saving model to weights.h5\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0027 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00661\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0025 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00661 to 0.00652, saving model to weights.h5\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.0024 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00652\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0064\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00652 to 0.00644, saving model to weights.h5\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0028 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00644\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0022 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00644\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0027 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00644\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0025 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00644\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0062\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00644 to 0.00620, saving model to weights.h5\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0022 - val_loss: 0.0063\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00620\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0061\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00620 to 0.00613, saving model to weights.h5\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0066\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00613\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0023 - val_loss: 0.0067\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00613\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0022 - val_loss: 0.0063\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00613\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0024 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00613 to 0.00604, saving model to weights.h5\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0020 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00604 to 0.00599, saving model to weights.h5\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0023 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00599\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00599\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0067\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00599\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00599 to 0.00593, saving model to weights.h5\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00593 to 0.00582, saving model to weights.h5\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0021 - val_loss: 0.0063\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00582\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0021 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00582\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0019 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00582\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0016 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00582 to 0.00579, saving model to weights.h5\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0023 - val_loss: 0.0071\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00579\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0024 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00579 to 0.00574, saving model to weights.h5\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00574\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0063\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00574\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0023 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00574 to 0.00568, saving model to weights.h5\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0023 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00568 to 0.00566, saving model to weights.h5\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00566 to 0.00563, saving model to weights.h5\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00563\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0064\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00563\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0020 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00563\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0017 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00563 to 0.00559, saving model to weights.h5\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00559\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00559 to 0.00554, saving model to weights.h5\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00554 to 0.00544, saving model to weights.h5\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0062\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00544\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00544\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0067\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00544\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0020 - val_loss: 0.0087\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00544\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0022 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00544\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0019 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00544\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00544 to 0.00541, saving model to weights.h5\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0018 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00541\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00541\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00541\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0058\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00541\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00541\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 1s 63ms/step - loss: 0.0019 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00541\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0019 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00541\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0019 - val_loss: 0.0062\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00541\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00541\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00541\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 54ms/step - loss: 0.0015 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00541\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00541 to 0.00530, saving model to weights.h5\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00530\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00530\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00530\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00530\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00530\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0017 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00530\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00530\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0017 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00530\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 1s 60ms/step - loss: 0.0017 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00530\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00530\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00530\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0018 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00530\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00530\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0021 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00530\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00530\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00530\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00530\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00530\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00530\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00530\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00530\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00530\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00530\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00530\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00530\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00530\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00530\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00530\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0013 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00530\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00530\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00530 to 0.00530, saving model to weights.h5\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00530\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00530\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00530\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00530\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00530\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0013 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00530 to 0.00530, saving model to weights.h5\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00530\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 2s 68ms/step - loss: 0.0014 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00530\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 1s 61ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00190: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00530\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00530\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00530\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00530\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0014 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00530\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00530\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00530\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00530\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00530\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 1s 57ms/step - loss: 0.0017 - val_loss: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00530\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.0014 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00530\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "history = regressor.fit(X_train, y_train, epochs = 200, callbacks=[rlr, mcp, tb], validation_data=(X_valid, y_valid), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting 246 values, using past 60 from the train data\n",
    "inputs = data2.iloc[len(data2) - len(valid_set) - 60:,1:3].values\n",
    "inputs = inputs.reshape(-inputs.shape[1],inputs.shape[1])\n",
    "inputs  = sc_valid.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(60,inputs.shape[0]):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (147,6) (2,) (147,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-49fd2b9ff78d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# invert scaling for forecast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosing_price\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclosing_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_yhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    430\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (147,6) (2,) (147,6) "
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "closing_price = regressor.predict(X_test)\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((closing_price,X_test[:,-1]), axis=1)\n",
    "closing_price = sc_valid.inverse_transform(inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69232106, 0.690907  , 0.69009173, 0.69529295, 0.69332445,\n",
       "        0.67382808],\n",
       "       [0.40480718, 0.403844  , 0.39815924, 0.41059008, 0.40539074,\n",
       "        0.32675411],\n",
       "       [0.40134442, 0.40189302, 0.39932966, 0.407875  , 0.40391946,\n",
       "        0.41623752],\n",
       "       [0.37678075, 0.37788805, 0.37662715, 0.38268036, 0.38000831,\n",
       "        0.37870713],\n",
       "       [0.40127742, 0.40241933, 0.40281332, 0.40578312, 0.40481809,\n",
       "        0.43173637],\n",
       "       [0.46004647, 0.46060351, 0.46255973, 0.46283877, 0.4633185 ,\n",
       "        0.50814704],\n",
       "       [0.37385845, 0.37368739, 0.37372428, 0.37665439, 0.37642634,\n",
       "        0.37564005],\n",
       "       [0.36188176, 0.36164695, 0.3617447 , 0.36467254, 0.36433309,\n",
       "        0.39217656],\n",
       "       [0.37893623, 0.37866783, 0.37928286, 0.38133913, 0.38129416,\n",
       "        0.39653659],\n",
       "       [0.36300895, 0.36269885, 0.36288834, 0.36533505, 0.36517897,\n",
       "        0.35758652],\n",
       "       [0.37160996, 0.37137586, 0.37169433, 0.37373722, 0.37374714,\n",
       "        0.371932  ],\n",
       "       [0.44048733, 0.44011649, 0.44163272, 0.44180131, 0.44254309,\n",
       "        0.45576427],\n",
       "       [0.43599412, 0.43538725, 0.43622261, 0.43708757, 0.4378069 ,\n",
       "        0.42079773],\n",
       "       [0.47457075, 0.47382343, 0.47502208, 0.47539362, 0.47624075,\n",
       "        0.48098962],\n",
       "       [0.41104269, 0.41044828, 0.40987021, 0.41248256, 0.41260141,\n",
       "        0.3761249 ],\n",
       "       [0.31541222, 0.31543562, 0.3130073 , 0.31831726, 0.31699917,\n",
       "        0.27850547],\n",
       "       [0.10235161, 0.10332811, 0.0967606 , 0.10862905, 0.10336956,\n",
       "        0.        ],\n",
       "       [0.07105082, 0.0736559 , 0.06902912, 0.07753289, 0.07378417,\n",
       "        0.0703059 ],\n",
       "       [0.09183031, 0.09509823, 0.09270759, 0.09657958, 0.09542021,\n",
       "        0.08794987],\n",
       "       [0.06324087, 0.06614502, 0.06405678, 0.0666741 , 0.06637816,\n",
       "        0.03381133],\n",
       "       [0.05860883, 0.06088623, 0.05958048, 0.06043939, 0.06118146,\n",
       "        0.06050227],\n",
       "       [0.02112517, 0.02244446, 0.02095073, 0.02221417, 0.02255765,\n",
       "        0.00142518],\n",
       "       [0.10984015, 0.1103304 , 0.11143054, 0.10840689, 0.11085331,\n",
       "        0.17838646],\n",
       "       [0.17727974, 0.1761791 , 0.17844093, 0.17423649, 0.17698102,\n",
       "        0.21139888],\n",
       "       [0.28108263, 0.27832994, 0.28200844, 0.27665985, 0.27950889,\n",
       "        0.33159532],\n",
       "       [0.32057244, 0.31646389, 0.31977412, 0.31612468, 0.31789351,\n",
       "        0.32721693],\n",
       "       [0.32215139, 0.31753424, 0.31980908, 0.31859094, 0.31892639,\n",
       "        0.31520022],\n",
       "       [0.2812677 , 0.27698311, 0.2776989 , 0.27935049, 0.27801228,\n",
       "        0.25389721],\n",
       "       [0.22246313, 0.21913458, 0.21846062, 0.22248441, 0.21960829,\n",
       "        0.18424329],\n",
       "       [0.23529819, 0.23309715, 0.23310354, 0.23602569, 0.23339804,\n",
       "        0.2269161 ],\n",
       "       [0.28709686, 0.28561983, 0.28679788, 0.2874814 , 0.28598803,\n",
       "        0.28598584],\n",
       "       [0.3281852 , 0.32695481, 0.32860535, 0.32803833, 0.32743767,\n",
       "        0.32253183],\n",
       "       [0.31230205, 0.31123209, 0.31203032, 0.31226817, 0.31165376,\n",
       "        0.28818238],\n",
       "       [0.24679448, 0.24613884, 0.24536566, 0.24771674, 0.24618439,\n",
       "        0.21330892],\n",
       "       [0.23323756, 0.23318931, 0.23258311, 0.23453501, 0.23304322,\n",
       "        0.23163426],\n",
       "       [0.27763009, 0.27791104, 0.27860376, 0.27825862, 0.27784175,\n",
       "        0.29214387],\n",
       "       [0.38247386, 0.38240081, 0.38502371, 0.38159481, 0.38263685,\n",
       "        0.41953601],\n",
       "       [0.42544255, 0.42464286, 0.42723909, 0.42372024, 0.42515308,\n",
       "        0.43367947],\n",
       "       [0.40408599, 0.40293637, 0.40423468, 0.40253368, 0.40343404,\n",
       "        0.39572115],\n",
       "       [0.48043486, 0.47906175, 0.48141834, 0.47861594, 0.47965121,\n",
       "        0.51644285],\n",
       "       [0.46066862, 0.45927405, 0.46042329, 0.4591493 , 0.45980689,\n",
       "        0.44625265],\n",
       "       [0.41733754, 0.41633713, 0.41622064, 0.41669771, 0.41656914,\n",
       "        0.40306194],\n",
       "       [0.33876452, 0.33851609, 0.33686247, 0.33951885, 0.33822969,\n",
       "        0.31018454],\n",
       "       [0.28140521, 0.28209114, 0.27992776, 0.28329933, 0.28137925,\n",
       "        0.26032889],\n",
       "       [0.27724603, 0.27871951, 0.27737722, 0.27919549, 0.27790862,\n",
       "        0.27186993],\n",
       "       [0.32696921, 0.32867232, 0.32891372, 0.32786635, 0.32803652,\n",
       "        0.34208216],\n",
       "       [0.38500896, 0.38626671, 0.38761833, 0.38462636, 0.38591182,\n",
       "        0.40736687],\n",
       "       [0.33481827, 0.33559328, 0.33547139, 0.33448979, 0.33523792,\n",
       "        0.31814058],\n",
       "       [0.30222121, 0.30292809, 0.30219847, 0.30231103, 0.30244598,\n",
       "        0.30971989],\n",
       "       [0.1711452 , 0.17210478, 0.16876897, 0.1730893 , 0.17091314,\n",
       "        0.12378694],\n",
       "       [0.16646692, 0.16818932, 0.16603687, 0.16846243, 0.16700363,\n",
       "        0.1836813 ],\n",
       "       [0.14937671, 0.15134971, 0.14958794, 0.15107214, 0.15010846,\n",
       "        0.13797081],\n",
       "       [0.18013844, 0.18205103, 0.1815342 , 0.18064104, 0.18102708,\n",
       "        0.1950901 ],\n",
       "       [0.18643638, 0.18781655, 0.18756881, 0.18612975, 0.18693769,\n",
       "        0.18671532],\n",
       "       [0.12089497, 0.12178133, 0.12017724, 0.12110615, 0.12067983,\n",
       "        0.08452098],\n",
       "       [0.13341987, 0.13419926, 0.13344373, 0.13316929, 0.13322639,\n",
       "        0.15449997],\n",
       "       [0.18906033, 0.18939933, 0.19004604, 0.1877356 , 0.18872759,\n",
       "        0.21787648],\n",
       "       [0.24651793, 0.24604908, 0.24760063, 0.24425377, 0.24571456,\n",
       "        0.26699015],\n",
       "       [0.51022398, 0.50789452, 0.51374459, 0.50563949, 0.50834394,\n",
       "        0.64036754],\n",
       "       [0.64054561, 0.63588083, 0.64196002, 0.63510203, 0.63752186,\n",
       "        0.69397897],\n",
       "       [0.6566813 , 0.65104699, 0.65510017, 0.65134084, 0.65330869,\n",
       "        0.6660208 ],\n",
       "       [0.74582684, 0.73982549, 0.74380243, 0.74117833, 0.74229825,\n",
       "        0.79683999],\n",
       "       [0.76678514, 0.76110697, 0.7638346 , 0.76269734, 0.76356035,\n",
       "        0.77056229],\n",
       "       [0.85937631, 0.85407174, 0.85719258, 0.85602462, 0.85642314,\n",
       "        0.886002  ],\n",
       "       [0.88252902, 0.87786376, 0.88001168, 0.87933648, 0.88003099,\n",
       "        0.88867789],\n",
       "       [0.89795053, 0.89394176, 0.89539647, 0.89503419, 0.89581418,\n",
       "        0.90990869],\n",
       "       [0.89340496, 0.89006746, 0.89085221, 0.89077461, 0.89156997,\n",
       "        0.90139249],\n",
       "       [0.85161591, 0.84895873, 0.84881139, 0.84935844, 0.85002339,\n",
       "        0.84915297],\n",
       "       [0.85669231, 0.85457122, 0.85479689, 0.85506427, 0.85528505,\n",
       "        0.86525606],\n",
       "       [0.87739933, 0.87566555, 0.8764087 , 0.87609434, 0.87614477,\n",
       "        0.89416924],\n",
       "       [0.86742485, 0.86596131, 0.86647177, 0.86618567, 0.8662895 ,\n",
       "        0.88545654],\n",
       "       [0.78147578, 0.7803387 , 0.77936602, 0.78053188, 0.78044748,\n",
       "        0.78355421],\n",
       "       [0.71794462, 0.71721423, 0.71579003, 0.71789747, 0.71709919,\n",
       "        0.71963408],\n",
       "       [0.77748585, 0.77708399, 0.77762645, 0.77780533, 0.77693295,\n",
       "        0.79178757],\n",
       "       [0.79816306, 0.79777402, 0.79875094, 0.79818559, 0.79776454,\n",
       "        0.81387605],\n",
       "       [0.74337077, 0.74298948, 0.74281001, 0.74333501, 0.74312437,\n",
       "        0.74961799],\n",
       "       [0.716479  , 0.71623027, 0.71581137, 0.71683693, 0.71643561,\n",
       "        0.72775174],\n",
       "       [0.69069469, 0.6906606 , 0.69011134, 0.69140375, 0.69090915,\n",
       "        0.69010197],\n",
       "       [0.69313264, 0.69327068, 0.69313586, 0.694013  , 0.69361037,\n",
       "        0.69319292],\n",
       "       [0.92408884, 0.92358923, 0.92742729, 0.92468107, 0.92423558,\n",
       "        1.        ],\n",
       "       [0.89893961, 0.89786339, 0.89977539, 0.89797354, 0.8992101 ,\n",
       "        0.91679033],\n",
       "       [0.91134238, 0.90982652, 0.91110516, 0.91011333, 0.91154552,\n",
       "        0.94804882],\n",
       "       [0.78376555, 0.78239787, 0.78092027, 0.7825954 , 0.78403854,\n",
       "        0.7589184 ],\n",
       "       [0.63098228, 0.63011718, 0.62647969, 0.63151181, 0.63133967,\n",
       "        0.58862756],\n",
       "       [0.65608513, 0.65593243, 0.6545803 , 0.6576885 , 0.65704608,\n",
       "        0.62813411],\n",
       "       [0.71023738, 0.71026695, 0.7108742 , 0.71166289, 0.71140981,\n",
       "        0.69997172],\n",
       "       [0.66070098, 0.66068316, 0.66054356, 0.66200221, 0.66195536,\n",
       "        0.6499416 ],\n",
       "       [0.64053488, 0.6405164 , 0.6403966 , 0.64206302, 0.64186656,\n",
       "        0.65371025],\n",
       "       [0.5257622 , 0.52595353, 0.52394247, 0.52817225, 0.52712381,\n",
       "        0.51263196],\n",
       "       [0.58786225, 0.5883894 , 0.58861113, 0.59016573, 0.58962739,\n",
       "        0.60688128],\n",
       "       [0.59867907, 0.59908974, 0.59967613, 0.600425  , 0.60043442,\n",
       "        0.60047898],\n",
       "       [0.65793192, 0.65798402, 0.65957189, 0.6589886 , 0.6595161 ,\n",
       "        0.67789426],\n",
       "       [0.67980969, 0.67941594, 0.68087983, 0.680287  , 0.68116331,\n",
       "        0.69374022],\n",
       "       [0.75574327, 0.7548337 , 0.75707042, 0.755741  , 0.75678027,\n",
       "        0.78531916],\n",
       "       [0.74322486, 0.74203348, 0.74314916, 0.74283183, 0.74412799,\n",
       "        0.74446089],\n",
       "       [0.69813299, 0.697025  , 0.69683146, 0.6979897 , 0.6990236 ,\n",
       "        0.68784849],\n",
       "       [0.77281427, 0.77175021, 0.77293324, 0.77286661, 0.7736637 ,\n",
       "        0.78041734],\n",
       "       [0.78271258, 0.78165066, 0.78254557, 0.78250575, 0.78355992,\n",
       "        0.7734255 ],\n",
       "       [0.81915438, 0.81801403, 0.81916267, 0.81884372, 0.81992221,\n",
       "        0.82556952],\n",
       "       [0.8577038 , 0.85644495, 0.85776782, 0.85720444, 0.85836577,\n",
       "        0.87100821],\n",
       "       [0.86599028, 0.86466348, 0.86556667, 0.86525464, 0.86657131,\n",
       "        0.87443525],\n",
       "       [0.78638947, 0.78525376, 0.78439975, 0.78569329, 0.78697062,\n",
       "        0.76975236],\n",
       "       [0.84470701, 0.84367573, 0.84428465, 0.84449863, 0.84523988,\n",
       "        0.85239636],\n",
       "       [0.88782597, 0.88675535, 0.88796651, 0.88745463, 0.88827693,\n",
       "        0.89675147],\n",
       "       [0.87200522, 0.8708781 , 0.87141806, 0.87131667, 0.87239718,\n",
       "        0.8762351 ],\n",
       "       [0.86677539, 0.8656199 , 0.86596417, 0.86613834, 0.86708796,\n",
       "        0.87706339],\n",
       "       [0.8611871 , 0.86006558, 0.8603245 , 0.86065531, 0.86145091,\n",
       "        0.86525422],\n",
       "       [0.86676097, 0.86567068, 0.86609161, 0.86633909, 0.86699021,\n",
       "        0.87013216],\n",
       "       [0.80092525, 0.799981  , 0.79927379, 0.80062497, 0.80118906,\n",
       "        0.7867223 ],\n",
       "       [0.83254611, 0.83170688, 0.83207488, 0.83265615, 0.83285666,\n",
       "        0.83748154],\n",
       "       [0.81486213, 0.81410587, 0.81426853, 0.8149426 , 0.81524754,\n",
       "        0.8104435 ],\n",
       "       [0.73761737, 0.7370162 , 0.73597068, 0.73802507, 0.73809862,\n",
       "        0.72383984],\n",
       "       [0.81903446, 0.81850755, 0.81955469, 0.81970906, 0.81960976,\n",
       "        0.83574598],\n",
       "       [0.84025133, 0.83958328, 0.84080082, 0.84048891, 0.8408488 ,\n",
       "        0.85129809],\n",
       "       [0.81377804, 0.81297982, 0.8134129 , 0.81378329, 0.81439567,\n",
       "        0.81921681],\n",
       "       [0.7837348 , 0.78295302, 0.78284734, 0.78392088, 0.78439832,\n",
       "        0.78586646],\n",
       "       [0.76357198, 0.76292342, 0.76269317, 0.76406586, 0.76433194,\n",
       "        0.75686511],\n",
       "       [0.75274777, 0.75225604, 0.75213522, 0.75347775, 0.75363004,\n",
       "        0.74249208],\n",
       "       [0.78156275, 0.78111768, 0.78175682, 0.78231126, 0.78251934,\n",
       "        0.78156153],\n",
       "       [0.79048479, 0.78998137, 0.79070532, 0.79103255, 0.79147518,\n",
       "        0.79223386],\n",
       "       [0.82735217, 0.82668376, 0.82789308, 0.82771111, 0.82829833,\n",
       "        0.84164689],\n",
       "       [0.84810185, 0.84724009, 0.84842223, 0.84816539, 0.84898961,\n",
       "        0.86158474],\n",
       "       [0.83956707, 0.83859181, 0.83923388, 0.83943224, 0.84041893,\n",
       "        0.84467908],\n",
       "       [0.92096984, 0.91977692, 0.92156053, 0.92087007, 0.92167127,\n",
       "        0.94651712],\n",
       "       [0.92066967, 0.91932762, 0.92039907, 0.92003739, 0.92132378,\n",
       "        0.92853155],\n",
       "       [0.90469933, 0.90327632, 0.90367854, 0.90390778, 0.90528035,\n",
       "        0.90807027],\n",
       "       [0.95425761, 0.9527539 , 0.95387489, 0.95365548, 0.95472646,\n",
       "        0.96902434],\n",
       "       [0.94537127, 0.9438504 , 0.94444734, 0.94449043, 0.94579053,\n",
       "        0.94689362],\n",
       "       [0.9116621 , 0.9101572 , 0.91009402, 0.91076672, 0.91201746,\n",
       "        0.90770296],\n",
       "       [0.92414117, 0.92267287, 0.92307508, 0.92356992, 0.92443681,\n",
       "        0.92741124],\n",
       "       [0.93025124, 0.92881763, 0.92941535, 0.92975783, 0.93051362,\n",
       "        0.93376395],\n",
       "       [0.94118881, 0.93973136, 0.94052476, 0.94072402, 0.94140935,\n",
       "        0.95209112],\n",
       "       [0.83214724, 0.83082306, 0.82969064, 0.83166265, 0.83238351,\n",
       "        0.81380075],\n",
       "       [0.84998155, 0.84880233, 0.84884274, 0.85023093, 0.85029101,\n",
       "        0.85217046],\n",
       "       [0.82141387, 0.82040668, 0.82036495, 0.82183456, 0.82183349,\n",
       "        0.8107098 ],\n",
       "       [0.75122285, 0.75038785, 0.74953973, 0.75206566, 0.75175965,\n",
       "        0.73711276],\n",
       "       [0.77422965, 0.77358103, 0.77391946, 0.77545643, 0.77495003,\n",
       "        0.77919786],\n",
       "       [0.75397176, 0.75342035, 0.75367725, 0.75517106, 0.75483704,\n",
       "        0.75138661],\n",
       "       [0.66214597, 0.6617707 , 0.66058987, 0.66379404, 0.66315341,\n",
       "        0.64767894],\n",
       "       [0.67564023, 0.67555213, 0.67542857, 0.67763174, 0.67694879,\n",
       "        0.67976022],\n",
       "       [0.55140013, 0.55163348, 0.54947263, 0.55407709, 0.55286425,\n",
       "        0.51811046],\n",
       "       [0.55765378, 0.55836666, 0.55743533, 0.56063044, 0.55963016,\n",
       "        0.55854448],\n",
       "       [0.55028051, 0.55117023, 0.55071807, 0.55301923, 0.55248904,\n",
       "        0.54348273],\n",
       "       [0.53286356, 0.53376138, 0.53335166, 0.53539693, 0.53516406,\n",
       "        0.52965333],\n",
       "       [0.55426919, 0.55506432, 0.55536652, 0.55636096, 0.55659747,\n",
       "        0.56430948],\n",
       "       [0.39151773, 0.39239895, 0.38934976, 0.39475337, 0.39357927,\n",
       "        0.34427687],\n",
       "       [0.36850509, 0.37002468, 0.36770391, 0.37224555, 0.37114638,\n",
       "        0.37568596]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_sma_rms=np.sqrt(np.mean(np.power((valid_set-closing_price),2)))\n",
    "lstm_sma_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting\n",
    "train = data2[:750]\n",
    "valid = data2[750:]\n",
    "valid['Predictions'] = closing_price[:,0]\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close','Predictions']])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend(labels=['Training Price', 'Actual Price', 'Predicted Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
