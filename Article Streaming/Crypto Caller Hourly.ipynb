{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Script to stream news articles and compare sentiment against cryptocurrency price\n",
    "    Author: Ross MacWilliam\n",
    "    Date: 01/02/2021\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shrimpy\n",
    "import plotly.graph_objects as go\n",
    "from newsapi import NewsApiClient\n",
    "from yahoofinancials import YahooFinancials\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'bitcoin'\n",
    "trading_symbol = 'BTC'\n",
    "\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'referer': 'https://www.google.com',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36 Edg/85.0.564.44'\n",
    "}\n",
    "date = datetime.today().strftime(\"%d%m%Y\")\n",
    "todays_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "yesterdays_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # NEWS API AUTHENTICATER # # # #\n",
    "class NewsAuthenticator():\n",
    "    \"\"\"\n",
    "    Functionality for authenticating News API\n",
    "    \"\"\"\n",
    "\n",
    "    def authenticate_news_api(self):\n",
    "        auth = NewsApiClient(api_key='cc3f4e3191f149658f3922e9c47ec1ad')\n",
    "        return auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # NEWS API STREAMER # # # #\n",
    "class NewsAPIArticleStreamer():\n",
    "    \"\"\"\n",
    "    Class for streaming and processing articles daily\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.news_authenticator = NewsAuthenticator()    \n",
    "\n",
    "    def stream_articles(self, query, from_param, to):\n",
    "        # This handles Twitter authetification and the connection to News API\n",
    "        newsapi = self.news_authenticator.authenticate_news_api()\n",
    "        \n",
    "        # Function to call all articles regarding query between set dates\n",
    "        all_articles = newsapi.get_everything(q=query,\n",
    "                                          from_param= from_param,\n",
    "                                          to= to,\n",
    "                                          language='en')\n",
    "        \n",
    "        return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAPIArticleCleaner():\n",
    "    \"\"\"\n",
    "    Functionality for importing and cleaning news api articles\n",
    "    \"\"\"\n",
    "    def import_json(self, fetched_json_file):\n",
    "        # Import json and normalize data\n",
    "        df = pd.read_json(fetched_json_file)\n",
    "        norm_articles = pd.json_normalize(df['articles'])\n",
    "        \n",
    "        return norm_articles\n",
    "\n",
    "    def clean_article(self, article):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", article).split())\n",
    "    \n",
    "    def create_article(self, articles):\n",
    "        article = articles[['author', 'title', 'description', 'url', 'source.name', 'publishedAt']]\n",
    "        article['publishedAt'] = pd.to_datetime(article['publishedAt'], infer_datetime_format=True)\n",
    "        article.sort_values(by='publishedAt',inplace=True)\n",
    "        article = article.reset_index()\n",
    "        article = article.drop(['index'], axis=1)\n",
    "        \n",
    "        outlet = articles[['source.id', 'source.name']]\n",
    "        \n",
    "        return article, outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # YAHOO FINANCE STREAMER # # # #\n",
    "class YahooArticleStreamer:\n",
    "    \"\"\"\n",
    "    Class for streaming and processing articles daily from Yahoo Finance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_article(self, card):\n",
    "        \"\"\"\n",
    "        Extract article information from the raw html\n",
    "        :param card: divs containing news articles\n",
    "        :return: articles\n",
    "        \"\"\"\n",
    "        headline = card.find('h4', 's-title').text\n",
    "        source = card.find(\"span\", 's-source').text\n",
    "        posted = card.find('span', 's-time').text.replace('Â·', '').strip()\n",
    "        description = card.find('p', 's-desc').text.strip()\n",
    "        raw_link = card.find('a').get('href')\n",
    "        unquoted_link = requests.utils.unquote(raw_link)\n",
    "        pattern = re.compile(r'RU=(.+)\\/RK')\n",
    "        clean_link = re.search(pattern, unquoted_link).group(1)\n",
    "\n",
    "        article = (headline, source, posted, description, clean_link)\n",
    "        return article\n",
    "\n",
    "    def get_the_news(self, query):\n",
    "        \"\"\"\n",
    "        Run beautiful soup to extract the html\n",
    "        :param query: query yahoo with this variable\n",
    "        :return all articles in dataframe format\n",
    "        \"\"\"\n",
    "        article_headers = ['title', 'outlet', 'uploaded', 'description', 'url']\n",
    "\n",
    "        template = 'https://news.search.yahoo.com/search?p={}'\n",
    "        url = template.format(query)\n",
    "        articles = []\n",
    "        links = set()\n",
    "\n",
    "        while True:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            cards = soup.find_all('div', 'NewsArticle')\n",
    "\n",
    "            # extract articles from page\n",
    "            for card in cards:\n",
    "                article = self.get_article(card)\n",
    "                link = article[-1]\n",
    "                if not link in links:\n",
    "                    links.add(link)\n",
    "                    articles.append(article)\n",
    "\n",
    "                    # find the next page\n",
    "            try:\n",
    "                url = soup.find('a', 'next').get('href')\n",
    "                sleep(1)\n",
    "            except AttributeError:\n",
    "                break\n",
    "\n",
    "        all_articles = pd.DataFrame(articles, columns=article_headers)\n",
    "\n",
    "        return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleCleaner():\n",
    "    \"\"\"\n",
    "    Functionality for importing and cleaning news articles\n",
    "    \"\"\"\n",
    "\n",
    "    def import_json(self, fetched_json_file):\n",
    "        \"\"\"\n",
    "        Import json and normalize data\n",
    "        :return normalized data\n",
    "        \"\"\"\n",
    "        df = pd.read_json(fetched_json_file)\n",
    "        norm_articles = pd.json_normalize(df['articles'])\n",
    "\n",
    "        return norm_articles\n",
    "\n",
    "    def clean_article(self, article):\n",
    "        \"\"\"\n",
    "        Clean dataframe of all 'dirty' symbols\n",
    "        \"\"\"\n",
    "        \n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", article).split())\n",
    "\n",
    "    def create_article(self, articles):\n",
    "        article = articles[['author', 'title', 'description', 'url', 'source.name', 'publishedAt']]\n",
    "        outlet = articles[['source.id', 'source.name']]\n",
    "\n",
    "        return article, outlet\n",
    "    \n",
    "    def get_recent_articles(self, all_articles):\n",
    "        recent_articles = pd.DataFrame()\n",
    "\n",
    "        for index, row in all_articles.iterrows():\n",
    "            if 'hour' in row.uploaded:\n",
    "                recent_articles = recent_articles.append(row)\n",
    "\n",
    "            if 'minute' in row.uploaded:\n",
    "                recent_articles = recent_articles.append(row)\n",
    "\n",
    "            if 'second' in row.uploaded:\n",
    "                recent_articles = recent_articles.append(row)\n",
    "\n",
    "        time = []\n",
    "        hours = recent_articles['uploaded'].str.split(' ')\n",
    "        df = pd.DataFrame(hours.values.tolist(), index=hours.index)\n",
    "        hours_ago = df[0]\n",
    "\n",
    "        for index, hours in hours_ago.items():\n",
    "            if int(hours) < 24:\n",
    "                d = datetime.today() - timedelta(hours=int(hours), minutes=0)\n",
    "                time.append(d.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            else:\n",
    "                d = datetime.today() - timedelta(hours=0, minutes=int(hours))\n",
    "                time.append(d.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        recent_articles['publishedAt'] = time\n",
    "        #recent_articles = recent_articles.drop(['uploaded'])\n",
    "\n",
    "        return recent_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer():\n",
    "    \"\"\"\n",
    "    Functionality for analyzing headlines sentiment\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('./datasets/headlines_labelled.txt',\n",
    "                        sep='\\t', header= None)\n",
    "        \n",
    "        # Add weighted words to lexicon\n",
    "        self.new_words = {\n",
    "            'crushes': 10,\n",
    "            'beats': 5,\n",
    "            'misses': -5,\n",
    "            'trouble': -10,\n",
    "            'falls': -100,\n",
    "            'slides': -50,\n",
    "            'slide': -50,\n",
    "            'record high': 15,\n",
    "            'low': -15,\n",
    "            'one week low': -30,\n",
    "            'worth more': 5,\n",
    "            'digital gold': 5,\n",
    "            'high': 15,\n",
    "            'cryptocurrency fund': 10,\n",
    "            'up': 5,\n",
    "            'soars': 70,\n",
    "            'rebound': 20,\n",
    "            'pullback': -40,\n",
    "            'slumps': -60,\n",
    "            'jumps': 50,\n",
    "            'record low': -100,\n",
    "            'soaring': 70,\n",
    "            'bearish': -50,\n",
    "            'bullish': 50,\n",
    "            'bulls': 10,\n",
    "            'bears' : -10,\n",
    "            'hodl': 10,\n",
    "            'pulls back': -40,\n",
    "            'selloff': -70,\n",
    "            'retrace': -70,\n",
    "            'drop': -50,\n",
    "            'buying': 10,\n",
    "            'selling': -10,\n",
    "            'rally': 15,\n",
    "            'bounces': 20,\n",
    "            'testing support': -5,\n",
    "            'climb': 5,\n",
    "            'rise': 20,\n",
    "            'crashes': -100,\n",
    "            'crash': -100,\n",
    "            'downward': -30,\n",
    "            'plunges': -100,\n",
    "            'plunge' : -80,\n",
    "            'cardano': 0,\n",
    "            'descends': -30,\n",
    "            'descend': -30,\n",
    "            'gain' : 20,\n",
    "            'gains' : 20,\n",
    "            'worst' : -25,\n",
    "            'loss' : -15,\n",
    "            'without risk': 10,\n",
    "            'tumbles': -50,\n",
    "            'jeopardy': -50,\n",
    "            'breakout' : 10\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def reading_dataset(self):\n",
    "        columnName = ['Headlines','Sentiment']\n",
    "        self.data.columns = columnName\n",
    "        self.data.head()\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    def analyze_test_headlines(self):\n",
    "        \"\"\"\n",
    "        A function to analyse test dataset and apply vader sentiment\n",
    "        :return analysed dataset\n",
    "        \"\"\"\n",
    "        # Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Update the lexicon\n",
    "        vader.lexicon.update(self.new_words)\n",
    "        \n",
    "        data = self.reading_dataset()\n",
    "        \n",
    "        # Iterate through the headlines and get the polarity scores\n",
    "        scores = data['Headlines'].apply(vader.polarity_scores)\n",
    "        \n",
    "        # Convert the list of dicts into a DataFrame\n",
    "        scores_df = pd.DataFrame.from_records(scores)\n",
    "\n",
    "        # Join the DataFrames\n",
    "        scored_news = data.join(scores_df)\n",
    "        \n",
    "        scored_news['assigned_label'] = scored_news['Sentiment'].apply(lambda Sentiment: 'pos' if Sentiment>0 else 'neg')\n",
    "        scored_news['predicted_label'] = scored_news['compound'].apply(lambda compound: 'pos' if compound>=0 else 'neg')\n",
    "        \n",
    "        return scored_news\n",
    "    \n",
    "    \n",
    "    def analyze_recent_headlines(self, data):    \n",
    "        \"\"\"\n",
    "        A function to analyse gathered dataset and apply vader sentiment\n",
    "        :return analysed dataset\n",
    "        \"\"\"\n",
    "        scores_df = pd.DataFrame()\n",
    "        # Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Update the lexicon\n",
    "        vader.lexicon.update(self.new_words)\n",
    "        \n",
    "        # Iterate through the headlines and get the polarity scores\n",
    "        #scores = data['clean_title'].apply(vader.polarity_scores)\n",
    "        \n",
    "        scores_df['neg'] = [vader.polarity_scores(x)['neg'] for x in data['clean_title']]\n",
    "        scores_df['neu'] = [vader.polarity_scores(x)['neu'] for x in data['clean_title']]\n",
    "        scores_df['pos'] = [vader.polarity_scores(x)['pos'] for x in data['clean_title']]\n",
    "        scores_df['compound'] = [vader.polarity_scores(x)['compound'] for x in data['clean_title']]\n",
    "        \n",
    "        # Convert the list of dicts into a DataFrame\n",
    "        #scores_df = pd.DataFrame.from_records(scores)\n",
    "\n",
    "        # Join the DataFrames\n",
    "        data = data.reset_index()\n",
    "        scored_news = data.merge(scores_df,left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        scored_news['predicted_label'] = scored_news['compound'].apply(lambda compound: 'pos' if compound > 0 else ('neu' if compound == 0 else 'neg'))\n",
    "        \n",
    "        return scored_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # SHRIMPY AUTHENTICATER # # # #\n",
    "class ShrimpyAuthenticator:\n",
    "    \"\"\"\n",
    "    Functionality for authenticating Shrimpy\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.public_key = '12326758a39a720e15d064cab3c1f0a9332d107de453bd41926bb3acd565059e'\n",
    "        self.secret_key = '6991cf4c9b518293429db0df6085d1731074bed8abccd7f0279a52fac5b0c1a8a2f6d28e11a50fbb1c6575d1407e637f9ad7c73fbddfa87c5d418fd58971f829'\n",
    "\n",
    "    def authenticate_shrimpy(self):\n",
    "        # create the client\n",
    "        client = shrimpy.ShrimpyApiClient(self.public_key, self.secret_key)\n",
    "        return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceStreamer:\n",
    "    \"\"\"\n",
    "    Functionality for constantly streaming BTC price (Legacy function)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.authenticator = ShrimpyAuthenticator()\n",
    "    \n",
    "    def parse_price(self):\n",
    "        res = requests.get('https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD&.tsrc=fin-srch')\n",
    "        soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        price = soup.find_all('span', class_='Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)')[0].text\n",
    "        price_change = soup.find_all('span', class_='Trsdu(0.3s) Fw(500) Pstart(10px) Fz(24px) C($positiveColor)')[0].text\n",
    "\n",
    "        return price, price_change\n",
    "    \n",
    "    def get_hist_data(self, ticker):\n",
    "        \"\"\"\n",
    "        Legacy function\n",
    "        \"\"\"\n",
    "        cryptocurrencies = ['BTC-USD', 'ETH-USD', 'ADA-USD']\n",
    "        yahoo_financials_cryptocurrencies = YahooFinancials(cryptocurrencies)\n",
    "\n",
    "        d7 = (datetime.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "        d13 = (datetime.today() - timedelta(days=13)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        daily_crypto_prices = yahoo_financials_cryptocurrencies.get_historical_price_data(d13, d7, 'daily')\n",
    "        data = pd.DataFrame.from_records(daily_crypto_prices)\n",
    "        price_df = pd.json_normalize(data[ticker]['prices'])\n",
    "        \n",
    "        return price_df\n",
    "    \n",
    "    def get_latest_prices(self, trading_symbol):\n",
    "        client = self.authenticator.authenticate_shrimpy()\n",
    "        \n",
    "        # get the candles for historical values\n",
    "        candles = client.get_candles(\n",
    "            'binance',  # exchange\n",
    "            trading_symbol,      # base_trading_symbol\n",
    "            'USDT',      # quote_trading_symbol\n",
    "            '1h'       # interval\n",
    "        )\n",
    "        \n",
    "        # Set the dataframe between these two dates\n",
    "        tomorrows_date = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        starting_date = '2021-01-23'\n",
    "\n",
    "        # Put pulled cryptocurrency values into a dataframe and set dates\n",
    "        prices_df = pd.DataFrame(candles)\n",
    "        prices_df['time'] = pd.to_datetime(prices_df['time'], infer_datetime_format=True).dt.tz_localize(None)\n",
    "\n",
    "        latest_prices = prices_df[(prices_df['time'] > starting_date) & (prices_df['time'] <tomorrows_date)]\n",
    "        latest_prices['close'] = latest_prices['close'].astype('float64')\n",
    "\n",
    "        return latest_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeCalls:\n",
    "    \"\"\"\n",
    "    Functionality for assessing sentiment, and applying trade calls, and making them into a visualisation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.price_streamer = PriceStreamer()\n",
    "    \n",
    "    def vizualise_calls(self,trading_symbol, sentiment_Buy, sentiment_Sell):\n",
    "        \"\"\"\n",
    "        Function to visualise trade calls on cryptocurrency price\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # construct the figure\n",
    "        fig = go.Figure(data=[go.Candlestick(x=latest_prices['time'],\n",
    "                               open=latest_prices['open'], high=latest_prices['high'],\n",
    "                               low=latest_prices['low'], close=latest_prices['close'])])\n",
    "\n",
    "        # display our graph\n",
    "        fig.show()\n",
    "        \n",
    "        \"\"\"\n",
    "        latest_prices = self.price_streamer.get_latest_prices(trading_symbol)\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(20, 10),dpi=80)\n",
    "        plt.plot(latest_prices['time'], latest_prices['close'],'-^', markevery=sentiment_Buy, ms=15, color='green')\n",
    "        plt.plot(latest_prices['time'], latest_prices['close'], '-^', markevery=sentiment_Sell, ms=15, color='red')\n",
    "        plt.plot(latest_prices['time'], latest_prices['close'])\n",
    "        plt.xlabel('Date',fontsize=14)\n",
    "        plt.ylabel('Price in Dollars', fontsize = 14)\n",
    "        plt.xticks(rotation='60',fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.title('Trade Calls - VADER', fontsize = 16)\n",
    "        plt.legend(['Buy','Sell','Close'])\n",
    "        plt.grid()\n",
    "        plt.show() \n",
    "    \n",
    "    \n",
    "    def make_trade_call(self, extreme_scores_df):\n",
    "        \"\"\"\n",
    "        Function to make trade calls with vader sentiment\n",
    "        \"\"\"\n",
    "        # VADER trade calls\n",
    "        sentiment_Buy= []\n",
    "        sentiment_Sell= []\n",
    "        extreme_scores_df['Date'] = pd.to_datetime(extreme_scores_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "        # Fill extreme scores with all dates indexed\n",
    "        tomorrows_date = (datetime.today() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        starting_date = '2021-01-23'\n",
    "\n",
    "        idx = pd.date_range(starting_date, tomorrows_date, freq='H')\n",
    "        extreme_scores_df = extreme_scores_df.set_index(extreme_scores_df['Date'])\n",
    "        extreme_scores_df = extreme_scores_df.reindex(idx,fill_value=0)\n",
    "        extreme_scores_df = extreme_scores_df.drop(['Date'], axis=1)\n",
    "        extreme_scores_df = extreme_scores_df.reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "\n",
    "        for i in range(len(extreme_scores_df)):\n",
    "            row=extreme_scores_df['Date'][i]\n",
    "            if extreme_scores_df['final_scores'].values[i] > 0:\n",
    "                #print(f\"Trade Call for {row} is Buy.\")\n",
    "                sentiment_Buy.append(extreme_scores_df['Date'].index[i]-19)\n",
    "            elif extreme_scores_df['final_scores'].values[i] < 0:\n",
    "                #print(f\"Trade Call for {row} is Sell.\")\n",
    "                sentiment_Sell.append(extreme_scores_df['Date'].index[i]-19)\n",
    "            \n",
    "        return sentiment_Buy, sentiment_Sell\n",
    "        \n",
    "    \n",
    "    def calculate_extreme_scores(self, high_headlines, trading_symbol):\n",
    "        high_headlines['times'] = pd.to_datetime(high_headlines['publishedAt'])\n",
    "        high_headlines['hour'] = high_headlines['times'].dt.date.astype(str) + ' ' +high_headlines['times'].dt.hour.astype(str)+ ':00'\n",
    "        grouped_dates = high_headlines.groupby([high_headlines['hour']])\n",
    "        keys_dates = list(grouped_dates.groups.keys())\n",
    "\n",
    "        max_cs = []\n",
    "        min_cs = []\n",
    "\n",
    "        for key in grouped_dates.groups.keys():\n",
    "            data = grouped_dates.get_group(key)\n",
    "            if data[\"compound\"].max() > .2:\n",
    "                max_cs.append(data[\"compound\"].max())\n",
    "            elif data[\"compound\"].max() < -.2:\n",
    "                max_cs.append(0)\n",
    "\n",
    "            if data[\"compound\"].min() < 0:\n",
    "                min_cs.append(data[\"compound\"].min())\n",
    "            elif data[\"compound\"].min() > 0:\n",
    "                min_cs.append(0)\n",
    "\n",
    "        extreme_scores_dict = {'Date':keys_dates,'max_scores':max_cs,'min_scores':min_cs}\n",
    "        extreme_scores_df = pd.DataFrame(extreme_scores_dict)\n",
    "\n",
    "        final_scores = []\n",
    "        for i in range(len(extreme_scores_df)):\n",
    "            final_scores.append(extreme_scores_df['max_scores'].values[i] + extreme_scores_df['min_scores'].values[i])\n",
    "\n",
    "        extreme_scores_df['final_scores'] = final_scores\n",
    "\n",
    "        \n",
    "        sentiment_Buy, sentiment_Sell = self.make_trade_call(extreme_scores_df)\n",
    "        \n",
    "        self.vizualise_calls(trading_symbol, sentiment_Buy, sentiment_Sell)\n",
    "        \n",
    "        return sentiment_Buy, sentiment_Sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    article_cleaner = ArticleCleaner()\n",
    "    yahoo_article_streamer = YahooArticleStreamer()\n",
    "    \n",
    "    news_api_streamer = NewsAPIArticleStreamer()\n",
    "    news_api_cleaner = NewsAPIArticleCleaner()\n",
    "    \n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "    trade_caller = TradeCalls()\n",
    "\n",
    "    #news_api_articles = news_api_streamer.stream_articles(query, yesterdays_date, todays_date)\n",
    "    #yahoo_articles = yahoo_article_streamer.get_the_news(query)\n",
    "        \n",
    "    news_article_data = pd.json_normalize(news_api_articles['articles'])\n",
    "    news_api_df, outlets = article_cleaner.create_article(news_article_data)\n",
    "    news_api_df['publishedAt'] = pd.to_datetime(news_api_df['publishedAt'], infer_datetime_format=True)\n",
    "    \n",
    "    yahoo_clean_title = np.array([article_cleaner.clean_article(article) for article in yahoo_articles['title']])\n",
    "    yahoo_clean_desc = np.array([article_cleaner.clean_article(article) for article in yahoo_articles['description']])\n",
    "    \n",
    "    news_api_clean_title = np.array([article_cleaner.clean_article(article) for article in news_api_df['title']])\n",
    "    news_api_clean_desc = np.array([article_cleaner.clean_article(article) for article in news_api_df['description']])\n",
    "    \n",
    "    yahoo_articles['clean_title'] = yahoo_clean_title\n",
    "    yahoo_articles['clean_description'] = yahoo_clean_desc\n",
    "    \n",
    "    news_api_df['clean_title'] = news_api_clean_title\n",
    "    news_api_df['clean_description'] = news_api_clean_desc\n",
    "    \n",
    "    recent_yahoo_articles = article_cleaner.get_recent_articles(yahoo_articles)\n",
    "    \n",
    "    all_headlines = recent_yahoo_articles[['publishedAt','clean_title']]\n",
    "    all_headlines = all_headlines.append(news_api_df[['publishedAt','clean_title']])\n",
    "\n",
    "    scored_headlines = sentiment_analyzer.analyze_recent_headlines(all_headlines)\n",
    "    \n",
    "    high_headlines = scored_headlines[(scored_headlines['compound'] > .5) | (scored_headlines['compound'] < -0.5)]\n",
    "    \n",
    "    high_headlines['date'] = pd.to_datetime(high_headlines['publishedAt'], format='%Y/%m/%d')\n",
    "    high_headlines['date'] = high_headlines['date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    high_headlines = high_headlines.drop(columns=['index'])\n",
    "    \n",
    "    #high_headlines.to_csv(f'.\\{trading_symbol}datasets\\{date}sentiment.csv', index=False)\n",
    "    #high_headlines.to_csv(f'.\\{trading_symbol}datasets\\overall_sentiment.csv', index=False, mode='a', header=False)\n",
    "    \n",
    "    #overall_headlines = pd.read_csv('.\\datasets\\overall_sentiment.csv')\n",
    "    \n",
    "    #sentiment_Buy, sentiment_Sell = trade_caller.calculate_extreme_scores(overall_headlines, trading_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_api_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-db643f8ad254>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#yahoo_articles = yahoo_article_streamer.get_the_news(query)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mnews_article_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_api_articles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'articles'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mnews_api_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutlets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_cleaner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_article_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnews_api_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publishedAt'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_api_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'publishedAt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_api_articles' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
